rm(list=objects())
library(opera)
library(mgcv)

source("R/ar_corr.R")
source("R/kalman_corr.R")
source("R/Brouillon_Joseph.R")
source('R/kao_functions.R')
source('R/function_eric.R')



f1 <- function(x)
{
  x^2+(0.5-x)^3
}

f2 <- function(x)
{
  x^2-0.5*(0.5-x)^3
}

f3 <- function(x)
{
  2*x-0.2*x^2-0.5*(0.5-x)^3
}


n <- 500
Nsimu <- 100
perf_simu <- NULL
regret_simu <- NULL
regret_simu_conv <- NULL

for(i  in c(1:Nsimu))
{
  X1 <- rnorm(n,0,1)
  X2 <- rnorm(n,0,1)
  X3 <- rnorm(n,0,1)
  sd1 <- X1^2+1
  sd2 <- X2^2+1
  sd3 <- X3^2+1
  eps1 <-  rnorm(n,0, sd1)
  eps2 <-  rnorm(n,0, sd2)
  eps3 <-  rnorm(n,0, sd3)
  Y1 <- f1(X1) + eps1
  Y2 <- f2(X2) + eps2
  Y3 <- f3(X3) + eps3
  
  Y <- 1/3*Y1+1/3*Y2+1/3*Y3
  
  
  Data <- data.frame(Y,Y1, Y2, X1, X2, X3,  f1=f1(X1), f2=f2(X2), f3=f3(X3), sd1=sd1, sd2=sd2)
  Data0 <- Data[1:(n/2),]
  Data1 <- Data[-c(1:(n/2)),]
  
  g0_a <- gam(c(Y1~s(X1), ~ s(X1)), data=Data0, family="gaulss")
  g0_b <- gam(c(Y2~s(X2), ~ s(X2)), data=Data0, family="gaulss")
  g0_c <- gam(c(Y2~s(X3), ~ s(X3)), data=Data0, family="gaulss")
  
  
  g0_a.forecast <- predict(g0_a, newdata=Data1)
  g0_b.forecast <- predict(g0_b, newdata=Data1)
  g0_c.forecast <- predict(g0_c, newdata=Data1)
  
  
  experts <- cbind(g0_a.forecast[,1], g0_b.forecast[,1], g0_c.forecast[,1])
  colnames(experts) <- c("gam_lss1","gam_lss2","gam_lss3")
  
  mes_exp <- apply(experts, 2, function(x)(mean((Data1$Y-x)^2)))
  
  kalmanPreMat <- experts
  kalmanPreVarMat <- cbind(exp(g0_a.forecast[,2])^2, exp(g0_b.forecast[,2])^2, exp(g0_c.forecast[,2])^2)
  
  ##########################################################################################
  ##################Oracle
  ##########################################################################################
  
  best_exp <-  mean((Data1$Y-experts[, which.min(mes_exp)])^2)
  best_conv <-  mean((Data1$Y-rowMeans(experts))^2)
  
  #test the oracle
  # or <- opera::oracle(Y = Data1$Y, experts=experts,  model = "convex")
  # or$coefficients
  # mean((Data1$Y-or$prediction)^2)
  ##########################################################################################
  ##################KAO
  ##########################################################################################
  
  ##sans gradient trick mais avec eta adaptatif (dans une grille)
  kao <- kaoGrid(Ft=kalmanPreVarMat, expertMatrix=kalmanPreMat, y=Data1$Y)
  kao_MSE <- mean((Data1$Y-kao$pred)^2)
  kao_regret <- mean((Data1$Y-kao$pred)^2 - (Data1$Y-experts[, which.min(mes_exp)])^2)
  kao_regret_conv <- mean((Data1$Y-kao$pred)^2 - (Data1$Y-rowMeans(experts))^2)
  
  ## avec gradient trick et eta adaptatif (dans une grille)
  kalman.kaoConv = kaoConv(FtMat=kalmanPreVarMat, expertMatrix=kalmanPreMat, y=Data1$Y)
  kaoConv_MSE <-mean((Data1$Y-kalman.kaoConv$pred)^2)
  kaoConv_regret <- mean((Data1$Y-kalman.kaoConv$pred)^2 - (Data1$Y-experts[, which.min(mes_exp)])^2)
  kaoConv_regret_conv <- mean((Data1$Y-kalman.kaoConv$pred)^2 - (Data1$Y-rowMeans(experts))^2)
  
  ## avec gradient trick, eta multiple et adaptatif (MLR)
  kalman.kao_GMLR = MLkao1(FtMat=kalmanPreVarMat, expertMatrix=kalmanPreMat, y=Data1$Y)
  kaoGMLR_MSE <-  mean((Data1$Y-kalman.kao_GMLR$pred)^2)
  kaoGMLR_MSE_regret <- mean((Data1$Y-kalman.kao_GMLR$pred)^2 - (Data1$Y-experts[, which.min(mes_exp)])^2)
  kaoGMLR_MSE_regret_conv <- mean((Data1$Y-kalman.kao_GMLR$pred)^2 - (Data1$Y-rowMeans(experts))^2)
  
  
  ##########################################################################################
  ##################BOA
  ##########################################################################################
  boa <- mixture(Y=Data1$Y, experts=experts, model = "BOA", loss.gradient=FALSE)
  boa_MSE<- mean((Data1$Y-boa$pred)^2)
  boa_regret <- mean((Data1$Y-boa$pred)^2 - (Data1$Y-experts[, which.min(mes_exp)])^2)
  boa_regret_conv <- mean((Data1$Y-boa$pred)^2 - (Data1$Y-rowMeans(experts))^2)
  
  
  boa_grad <- mixture(Y=Data1$Y, experts=experts, model = "BOA", loss.gradient = TRUE)
  boa_grad_MSE <- mean((Data1$Y-boa_grad$pred)^2)
  boa_grad_regret <- mean((Data1$Y-boa_grad$pred)^2 - (Data1$Y-experts[, which.min(mes_exp)])^2)
  boa_grad_regret_conv <- mean((Data1$Y-boa_grad$pred)^2 - (Data1$Y-rowMeans(experts))^2)
  
  
  ##########################################################################################
  ##################MLpol
  ##########################################################################################
  mlpol <- mixture(Y=Data1$Y, experts=experts, model = "MLpol", loss.gradient=FALSE)
  mlpol_MSE<- mean((Data1$Y-mlpol$pred)^2)
  mlpol_regret <- mean((Data1$Y-mlpol$pred)^2 - (Data1$Y-experts[, which.min(mes_exp)])^2)
  mlpol_regret_conv <- mean((Data1$Y-mlpol$pred)^2 - (Data1$Y-rowMeans(experts))^2)
  
  
  mlpol_grad <- mixture(Y=Data1$Y, experts=experts, model = "MLpol", loss.gradient = TRUE)
  mlpol_grad_MSE <- mean((Data1$Y-mlpol_grad$pred)^2)
  mlpol_grad_regret <- mean((Data1$Y-mlpol_grad$pred)^2 - (Data1$Y-experts[, which.min(mes_exp)])^2)
  mlpol_grad_regret_conv <- mean((Data1$Y-mlpol_grad$pred)^2 - (Data1$Y-rowMeans(experts))^2)
  
  

  # perf  <-  c(kao_MSE, kaoConv_MSE, kaoGMLR_MSE, boa_MSE, boa_grad_MSE, mlpol_MSE, mlpol_grad_MSE)
  # names(perf) <- c("kao", "kaoConv", "kaoGMLR", "boa", "boa_grad", "mlpol", "mlpol_grad")
  # sort(perf)
  # 
  # regret  <-  c(kao_regret, kaoConv_regret, kaoGMLR_MSE_regret, boa_regret, boa_grad_regret, mlpol_regret, mlpol_grad_regret)
  # names(regret) <- c("kao", "kaoConv", "kaoGMLR", "boa", "boa_grad", "mlpol", "mlpol_grad")
  # sort(regret)
  
  
  perf  <-  c(kao_MSE, kaoConv_MSE, kaoGMLR_MSE, boa_MSE, boa_grad_MSE, mlpol_MSE, mlpol_grad_MSE, best_exp, best_conv)
  names(perf) <- c("kao", "kaoConv", "kaoGMLR", "boa", "boa_grad", "mlpol", "mlpol_grad", "best_exp","best_conv")
  sort(perf)
  
  regret  <-  c(kao_regret, kaoConv_regret, kaoGMLR_MSE_regret, boa_regret, boa_grad_regret, mlpol_regret, mlpol_grad_regret)
  names(regret) <- c("kao", "kaoConv", "kaoGMLR", "boa", "boa_grad", "mlpol", "mlpol_grad")
  sort(regret)
  
  regret_conv  <-  c(kao_regret_conv, kaoConv_regret_conv, kaoGMLR_MSE_regret_conv, boa_regret_conv, boa_grad_regret_conv, mlpol_regret_conv, mlpol_grad_regret_conv)
  names(regret_conv) <- c("kao", "kaoConv", "kaoGMLR", "boa", "boa_grad", "mlpol", "mlpol_grad")
  
  
  perf_simu <- rbind(perf_simu, perf)
  regret_simu <- rbind(regret_simu, regret)
  regret_simu_conv <- rbind(regret_simu_conv, regret_conv)
  
  print(i)

}


res <- list(MSE=perf_simu, regret=regret_simu, regret_conv=regret_simu_conv)

saveRDS(res, "Results/simu1_iid_mlpol_V2.RDS")







plot(boa)

matplot(kao$weight, type='l', col=c("blue", "red"), lty=1)
matplot(kalman.kaoConv$weight,  type='l', col=c("blue", "red"), lty=1)
matplot(kalman.kao_GMLR$weight, type='l', col=c("blue", "red"), lty=1)













